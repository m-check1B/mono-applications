#!/usr/bin/env python3
"""Provider Health Probes Test Suite

Tests the provider health monitoring and probing system:
- Health check execution
- Performance metrics collection
- Status determination logic
- Alerting thresholds
- Historical data tracking
"""

import asyncio
import logging
import sys
import time
from typing import Dict, Any

# Add the backend directory to Python path
sys.path.insert(0, '/home/adminmatej/github/applications/operator-demo-2026/backend')

from app.services.provider_health_monitor import (
    get_health_monitor,
    ProviderHealthConfig,
    ProviderStatus,
    HealthCheckResult,
    ProviderMetrics,
    ProviderHealthMonitor,
    ProviderType
)
from datetime import datetime, timezone

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestProviderHealthProbes:
    """Test suite for provider health probes."""
    
    def __init__(self):
        self.test_results = []
        self.health_monitor = None
    
    def run_test(self, test_name: str, test_func):
        """Run a test and record the result."""
        try:
            result = test_func()
            status = "‚úì PASS" if result else "‚úó FAIL"
            logger.info(f"{test_name}: {status}")
            self.test_results.append((test_name, result, None))
            return result
        except Exception as e:
            logger.error(f"{test_name}: ‚úó FAIL - {e}")
            self.test_results.append((test_name, False, str(e)))
            return False
    
    async def run_async_test(self, test_name: str, test_func):
        """Run an async test and record the result."""
        try:
            result = await test_func()
            status = "‚úì PASS" if result else "‚úó FAIL"
            logger.info(f"{test_name}: {status}")
            self.test_results.append((test_name, result, None))
            return result
        except Exception as e:
            logger.error(f"{test_name}: ‚úó FAIL - {e}")
            self.test_results.append((test_name, False, str(e)))
            return False
    
    def test_health_monitor_initialization(self):
        """Test health monitor initialization and configuration."""
        try:
            # Test default configuration
            monitor = get_health_monitor()
            assert monitor is not None
            assert monitor.config.check_interval_seconds == 30
            assert monitor.config.timeout_seconds == 5
            assert monitor.config.latency_warning_threshold_ms == 1000
            assert monitor.config.latency_error_threshold_ms == 3000
            
            # Test custom configuration
            custom_config = ProviderHealthConfig(
                check_interval_seconds=10,
                timeout_seconds=3,
                latency_warning_threshold_ms=500,
                latency_error_threshold_ms=1500
            )
            
            custom_monitor = ProviderHealthMonitor(custom_config)
            assert custom_monitor.config.check_interval_seconds == 10
            assert custom_monitor.config.timeout_seconds == 3
            assert custom_monitor.config.latency_warning_threshold_ms == 500
            assert custom_monitor.config.latency_error_threshold_ms == 1500
            
            logger.info("‚úì Health monitor initialization validated")
            return True
            
        except Exception as e:
            logger.error(f"Health monitor initialization test failed: {e}")
            return False
    
    def test_provider_configuration(self):
        """Test provider configuration registry."""
        try:
            monitor = get_health_monitor()
            configs = monitor._provider_configs
            
            # Check required providers are configured
            required_providers = ["gemini", "openai", "deepgram_nova3", "twilio", "telnyx"]
            for provider_id in required_providers:
                assert provider_id in configs, f"Missing provider: {provider_id}"
                
                config = configs[provider_id]
                assert "type" in config
                assert "endpoint" in config
                assert "enabled" in config
                assert config["enabled"] is True
            
            logger.info(f"‚úì Provider configuration validated for {len(configs)} providers")
            return True
            
        except Exception as e:
            logger.error(f"Provider configuration test failed: {e}")
            return False
    
    async def test_health_check_execution(self):
        """Test individual health check execution."""
        try:
            monitor = get_health_monitor()
            
            # Test health check for a specific provider
            provider_config = monitor._provider_configs["gemini"]
            result = await monitor._check_provider("gemini", provider_config)
            
            # Validate result structure
            assert isinstance(result, HealthCheckResult)
            assert result.provider_id == "gemini"
            assert result.provider_type is not None
            assert result.status in [s.value for s in ProviderStatus]
            assert isinstance(result.latency_ms, (int, float))
            assert isinstance(result.response_time, (int, float))
            assert isinstance(result.success, bool)
            assert result.timestamp is not None
            
            logger.info(f"‚úì Health check executed for {result.provider_id}: {result.status} ({result.latency_ms:.1f}ms)")
            return True
            
        except Exception as e:
            logger.error(f"Health check execution test failed: {e}")
            return False
    
    async def test_status_determination_logic(self):
        """Test status determination based on performance metrics."""
        try:
            monitor = get_health_monitor()
            
            # Test different latency scenarios
            test_cases = [
                (50, ProviderStatus.HEALTHY),    # Fast response
                (800, ProviderStatus.HEALTHY),   # Acceptable response
                (1200, ProviderStatus.DEGRADED), # Slow but working
                (2000, ProviderStatus.DEGRADED), # Degraded performance
                (4000, ProviderStatus.UNHEALTHY) # Too slow
            ]
            
            for latency_ms, expected_status in test_cases:
                # Create a mock result
                result = HealthCheckResult(
                    provider_id="test",
                    provider_type=ProviderType.GEMINI,
                    status=ProviderStatus.HEALTHY,  # Will be updated by _update_metrics
                    latency_ms=latency_ms,
                    timestamp=datetime.now(timezone.utc),
                    response_time=latency_ms / 1000,
                    success=True
                )
                
                # Add to history and update metrics
                monitor._store_check_result(result)
                monitor._update_metrics("test")
                
                # Check the determined status
                metrics = monitor.get_provider_health("test")
                assert metrics is not None
                
                # The status determination considers multiple factors, so we check the trend
                logger.info(f"‚úì Latency {latency_ms}ms -> Status: {metrics.status}")
            
            return True
            
        except Exception as e:
            logger.error(f"Status determination logic test failed: {e}")
            return False
    
    async def test_consecutive_failure_tracking(self):
        """Test consecutive failure tracking and automatic status changes."""
        try:
            monitor = get_health_monitor()
            provider_id = "test_failures"
            
            # Simulate multiple failures
            for i in range(5):
                result = HealthCheckResult(
                    provider_id=provider_id,
                    provider_type=ProviderType.GEMINI,
                    status=ProviderStatus.OFFLINE,
                    latency_ms=5000,
                    timestamp=datetime.now(timezone.utc),
                    response_time=5.0,
                    success=False,
                    error_message=f"Simulated failure {i+1}"
                )
                
                monitor._store_check_result(result)
                monitor._update_metrics(provider_id)
            
            # Check consecutive failures
            metrics = monitor.get_provider_health(provider_id)
            assert metrics is not None
            assert metrics.consecutive_failures >= 5
            assert metrics.status == ProviderStatus.OFFLINE
            
            # Simulate recovery
            success_result = HealthCheckResult(
                provider_id=provider_id,
                provider_type="test",
                status=ProviderStatus.HEALTHY,
                latency_ms=100,
                timestamp=time.time(),
                response_time=0.1,
                success=True
            )
            
            monitor._store_check_result(success_result)
            monitor._update_metrics(provider_id)
            
            # Check recovery
            recovered_metrics = monitor.get_provider_health(provider_id)
            assert recovered_metrics.consecutive_failures == 0
            
            logger.info(f"‚úì Consecutive failure tracking: {metrics.consecutive_failures} failures -> recovery")
            return True
            
        except Exception as e:
            logger.error(f"Consecutive failure tracking test failed: {e}")
            return False
    
    async def test_metrics_aggregation(self):
        """Test metrics aggregation and calculation."""
        try:
            monitor = get_health_monitor()
            provider_id = "test_metrics"
            
            # Simulate multiple health checks with varying performance
            test_data = [
                (100, True),   # Fast success
                (200, True),   # Normal success
                (150, True),   # Good success
                (5000, False), # Failure
                (300, True),   # Success
                (800, True),   # Slow success
            ]
            
            for latency_ms, success in test_data:
                result = HealthCheckResult(
                    provider_id=provider_id,
                    provider_type="test",
                    status=ProviderStatus.HEALTHY if success else ProviderStatus.OFFLINE,
                    latency_ms=latency_ms,
                    timestamp=time.time(),
                    response_time=latency_ms / 1000,
                    success=success,
                    error_message=None if success else "Test failure"
                )
                
                monitor._store_check_result(result)
            
            # Update metrics
            monitor._update_metrics(provider_id)
            
            # Check aggregated metrics
            metrics = monitor.get_provider_health(provider_id)
            assert metrics is not None
            assert metrics.total_checks == len(test_data)
            assert metrics.successful_checks == sum(1 for _, success in test_data if success)
            assert metrics.failed_checks == sum(1 for _, success in test_data if not success)
            
            expected_success_rate = (metrics.successful_checks / metrics.total_checks) * 100
            assert abs(metrics.success_rate - expected_success_rate) < 0.1
            
            # Check latency calculations (only successful checks)
            successful_latencies = [latency for latency, success in test_data if success]
            expected_avg_latency = sum(successful_latencies) / len(successful_latencies)
            assert abs(metrics.average_latency_ms - expected_avg_latency) < 1.0
            
            logger.info(f"‚úì Metrics aggregation: {metrics.total_checks} checks, {metrics.success_rate:.1f}% success, {metrics.average_latency_ms:.1f}ms avg latency")
            return True
            
        except Exception as e:
            logger.error(f"Metrics aggregation test failed: {e}")
            return False
    
    async def test_health_monitor_lifecycle(self):
        """Test health monitor start/stop lifecycle."""
        try:
            monitor = get_health_monitor()
            
            # Ensure monitor is stopped initially
            await monitor.stop()
            assert monitor._running is False
            
            # Start monitoring
            await monitor.start()
            assert monitor._running is True
            assert monitor._monitor_task is not None
            
            # Let it run for a short time
            await asyncio.sleep(0.2)
            
            # Check that some health data was collected
            all_health = monitor.get_all_providers_health()
            assert len(all_health) > 0
            
            # Stop monitoring
            await monitor.stop()
            assert monitor._running is False
            
            logger.info(f"‚úì Health monitor lifecycle tested, collected data for {len(all_health)} providers")
            return True
            
        except Exception as e:
            logger.error(f"Health monitor lifecycle test failed: {e}")
            return False
    
    def test_healthy_providers_filtering(self):
        """Test filtering of healthy providers."""
        try:
            monitor = get_health_monitor()
            
            # Get all providers
            all_providers = monitor.get_all_providers_health()
            assert len(all_providers) > 0
            
            # Get healthy providers
            healthy_providers = monitor.get_healthy_providers()
            assert isinstance(healthy_providers, list)
            
            # Verify all returned providers are actually healthy
            for provider_id in healthy_providers:
                metrics = monitor.get_provider_health(provider_id)
                assert metrics is not None
                assert metrics.status == ProviderStatus.HEALTHY
            
            # Test individual provider health check
            for provider_id in all_providers.keys():
                is_healthy = monitor.is_provider_healthy(provider_id)
                metrics = monitor.get_provider_health(provider_id)
                expected_healthy = metrics.status == ProviderStatus.HEALTHY
                assert is_healthy == expected_healthy
            
            logger.info(f"‚úì Healthy providers filtering: {len(healthy_providers)}/{len(all_providers)} healthy")
            return True
            
        except Exception as e:
            logger.error(f"Healthy providers filtering test failed: {e}")
            return False
    
    async def test_alerting_thresholds(self):
        """Test alerting threshold configurations."""
        try:
            # Test custom alerting configuration
            alert_config = ProviderHealthConfig(
                check_interval_seconds=5,
                timeout_seconds=2,
                max_consecutive_failures=2,
                latency_warning_threshold_ms=200,
                latency_error_threshold_ms=500,
                history_retention_hours=1
            )
            
            monitor = ProviderHealthMonitor(alert_config)
            
            # Test warning threshold
            warning_result = HealthCheckResult(
                provider_id="test_warning",
                provider_type="test",
                status=ProviderStatus.HEALTHY,
                latency_ms=300,  # Above warning threshold
                timestamp=time.time(),
                response_time=0.3,
                success=True
            )
            
            monitor._store_check_result(warning_result)
            monitor._update_metrics("test_warning")
            
            warning_metrics = monitor.get_provider_health("test_warning")
            assert warning_metrics.status == ProviderStatus.DEGRADED
            
            # Test error threshold
            error_result = HealthCheckResult(
                provider_id="test_error",
                provider_type="test",
                status=ProviderStatus.HEALTHY,
                latency_ms=600,  # Above error threshold
                timestamp=time.time(),
                response_time=0.6,
                success=True
            )
            
            monitor._store_check_result(error_result)
            monitor._update_metrics("test_error")
            
            error_metrics = monitor.get_provider_health("test_error")
            assert error_metrics.status == ProviderStatus.UNHEALTHY
            
            logger.info("‚úì Alerting thresholds: Warning and error thresholds validated")
            return True
            
        except Exception as e:
            logger.error(f"Alerting thresholds test failed: {e}")
            return False
    
    async def run_all_tests(self):
        """Run all tests."""
        logger.info("=== Provider Health Probes Test Suite ===\n")
        
        # Synchronous tests
        self.run_test("Health Monitor Initialization", self.test_health_monitor_initialization)
        self.run_test("Provider Configuration", self.test_provider_configuration)
        self.run_test("Healthy Providers Filtering", self.test_healthy_providers_filtering)
        
        # Asynchronous tests
        await self.run_async_test("Health Check Execution", self.test_health_check_execution)
        await self.run_async_test("Status Determination Logic", self.test_status_determination_logic)
        await self.run_async_test("Consecutive Failure Tracking", self.test_consecutive_failure_tracking)
        await self.run_async_test("Metrics Aggregation", self.test_metrics_aggregation)
        await self.run_async_test("Health Monitor Lifecycle", self.test_health_monitor_lifecycle)
        await self.run_async_test("Alerting Thresholds", self.test_alerting_thresholds)
        
        # Summary
        total_tests = len(self.test_results)
        passed_tests = sum(1 for _, result, _ in self.test_results if result)
        failed_tests = total_tests - passed_tests
        
        logger.info(f"\n=== Test Summary ===")
        logger.info(f"Total tests: {total_tests}")
        logger.info(f"Passed: {passed_tests}")
        logger.info(f"Failed: {failed_tests}")
        logger.info(f"Success rate: {(passed_tests/total_tests)*100:.1f}%")
        
        if failed_tests > 0:
            logger.info("\nFailed tests:")
            for name, result, error in self.test_results:
                if not result:
                    logger.info(f"  - {name}: {error}")
        
        return passed_tests == total_tests


async def main():
    """Main test runner."""
    tester = TestProviderHealthProbes()
    success = await tester.run_all_tests()
    
    if success:
        logger.info("\nüéâ All provider health probe tests passed!")
        logger.info("‚úì Health check execution validated")
        logger.info("‚úì Performance metrics collection verified")
        logger.info("‚úì Status determination logic tested")
        logger.info("‚úì Alerting thresholds confirmed")
        logger.info("‚úì Historical data tracking working")
        return 0
    else:
        logger.error("\n‚ùå Some provider health probe tests failed!")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)