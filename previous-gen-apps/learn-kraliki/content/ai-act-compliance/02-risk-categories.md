# Understanding Risk Categories

## The Risk-Based Framework

The EU AI Act classifies AI systems into four risk categories:

| Category | Description | Compliance Burden | Examples |
|----------|-------------|-------------------|----------|
| **Unacceptable Risk** | Prohibited outright | Banned | Social scoring, real-time biometric ID in public |
| **High Risk** | Strict requirements | Heavy | Medical devices, recruitment, critical infrastructure |
| **Limited Risk** | Transparency obligations | Light | Chatbots, deepfakes, emotion recognition |
| **Minimal Risk** | No specific requirements | None | AI-enabled video games, spam filters |

## Unacceptable Risk - Prohibited AI Systems

These AI systems are **completely banned** in the EU:

### 1. Subliminal Manipulation
AI that uses subliminal techniques to materially distort behavior
- Purpose: To exploit vulnerabilities of specific groups
- Impact: Deprives users of informed consent

### 2. Exploitation of Vulnerabilities
AI that exploits vulnerabilities due to:
- Age
- Disability
- Socio-economic status

### 3. Social Scoring
AI systems for evaluating or classifying:
- Natural persons or groups
- Based on social behavior or socio-economic status
- Leading to detrimental treatment

### 4. Real-Time Biometric Identification in Public Spaces
Using biometric identification systems in real-time for:
- Law enforcement purposes (with limited exceptions)
- Exception: Only for court-ordered investigations of serious crimes

### 5. Predictive Policing
AI that evaluates individual risk based solely on profiling
- Based on characteristics like ethnicity, religion, or gender

### 6. Emotional Recognition in Workplaces/Education
AI that infers emotions in:
- Educational settings
- Workplace environments

## High-Risk AI Systems

High-risk AI systems are those that:
1. Pose significant risk to health, safety, or fundamental rights
2. Are listed in **Annex III** of the AI Act
3. Are safety components of products regulated under EU product safety legislation

### High-Risk Use Cases (Annex III)

| Sector | Examples |
|--------|----------|
| **Biometrics** | Remote biometric identification, emotion recognition |
| **Critical Infrastructure** | Water, energy, transport management systems |
| **Education** | Vocational training, admission decisions |
| **Employment** | Recruitment, worker management, promotion |
| **Essential Services** | Access to healthcare, banking, insurance |
| **Law Enforcement** | Crime prediction, evidence evaluation |
| **Migration & Asylum** | Border control, visa decisions |
| **Administration of Justice** | Legal research, sentencing assistance |

### Requirements for High-Risk AI

High-risk AI systems must:
- ✅ Have a **risk management system**
- ✅ Undergo **conformity assessment** (third-party certification)
- ✅ Use **high-quality training data**
- ✅ Provide **technical documentation**
- ✅ Enable **record-keeping and logging**
- ✅ Ensure **transparency and provision of information to users**
- ✅ Provide **human oversight**
- ✅ Ensure **accuracy, robustness, and cybersecurity**
- ✅ Have a **quality management system**
- ✅ Include a **post-market monitoring system**

## Limited Risk AI Systems

These systems require **transparency obligations** but no prior approval:

### Examples

1. **Chatbots and virtual assistants**
   - Must disclose AI nature to users

2. **Deepfakes and AI-generated content**
   - Must be labeled as AI-generated or manipulated

3. **Emotion recognition systems** (outside workplace/education)
   - Must inform users they are being analyzed

4. **Biometric categorization systems**
   - Must disclose use to data subjects

### Compliance Requirements

- Users must be informed they are interacting with AI
- AI-generated or manipulated content must be disclosed
- Clear labeling of synthetic media

## Minimal Risk AI Systems

No specific requirements under the AI Act.

### Examples

- AI-enabled video games
- Spam filters
- AI in consumer appliances
- Content recommendation systems

**Note:** Even minimal-risk AI must comply with existing laws (GDPR, product safety, etc.)

## Classification Exercise

For each AI system in your organization:

1. What is its intended use?
2. Who uses it? (public, employees, customers)
3. What decisions does it make or influence?
4. Could it cause harm if it fails?
5. Does it process sensitive personal data?

Based on these answers, determine which risk category applies.

---

**Next Lesson:** Compliance Timeline & Deadlines
