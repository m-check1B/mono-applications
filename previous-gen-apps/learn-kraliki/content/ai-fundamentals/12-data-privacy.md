# Data Privacy: What Never to Upload

When you type something into an AI system, where does it go?

This lesson is about protecting yourself, your company, and the people who trust you with their information.

---

## The Reality of AI Data

Most AI systems work like this:

1. You type a message
2. It goes to the company's servers
3. It's processed to generate a response
4. It may be stored and used for training

The key question: **Who sees what you type?**

---

## What Happens to Your Data?

This varies by service and settings, but generally:

### Default Mode (Free Accounts)
- Your conversations may be stored
- Data may be used to train future models
- Humans may review conversations for quality
- Data may be retained for months or years

### Privacy Mode (Some Paid Plans)
- Conversations not used for training
- Shorter or no retention
- Opt-out available
- Enterprise controls

**Always check the privacy policy of any AI tool you use.**

---

## The "Newspaper Test"

Before typing anything sensitive, ask yourself:

> **"Would I be comfortable if this appeared on the front page of a newspaper with my name attached?"**

If not, don't type it.

---

## Categories of Sensitive Data

### NEVER Share: Personal Identifiable Information (PII)

| Data Type | Example | Risk |
|-----------|---------|------|
| Full names | "John Smith at 123 Main St" | Identity theft |
| Social Security / ID numbers | SSN, passport numbers | Fraud |
| Financial details | Bank accounts, card numbers | Financial theft |
| Medical records | Diagnoses, prescriptions | Privacy violation |
| Passwords | Login credentials | Account compromise |
| Addresses | Home or work location | Physical safety |

### HIGH RISK: Company Confidential

| Data Type | Example | Risk |
|-----------|---------|------|
| Trade secrets | Proprietary formulas, processes | Competitive harm |
| Financial data | Revenue, budgets, forecasts | Market manipulation |
| Strategy documents | M&A plans, product roadmaps | Competitive leak |
| Customer data | Client lists, contracts | Legal liability |
| Source code | Proprietary algorithms | IP theft |
| Internal communications | Private emails, memos | Reputation damage |

### MODERATE RISK: Professional Data

| Data Type | Example | Risk |
|-----------|---------|------|
| Internal policies | HR policies, procedures | Minor exposure |
| Performance data | Reviews, metrics | Privacy issues |
| Project details | Timelines, team info | Competitive intel |

---

## The Anonymization Solution

You can often use AI safely by removing identifying information:

### Before (Dangerous):
```
"Write a performance review for John Smith, our
Senior Developer at Acme Corp who makes $150,000
and has been struggling with attendance since his
divorce last March."
```

### After (Safe):
```
"Write a performance review for a senior developer
who has been struggling with attendance due to
personal circumstances over the past 6 months."
```

Same output. No exposure.

---

## Anonymization Techniques

### Technique 1: Remove Names
Replace: "John Smith" with "the employee" or "[NAME]"

### Technique 2: Remove Numbers
Replace: "$150,000 salary" with "senior-level compensation"

### Technique 3: Generalize Details
Replace: "Acme Corp" with "a mid-size tech company"

### Technique 4: Change Identifying Combos
Replace: "VP of Marketing at a solar company in Austin" with "a marketing leader at a company"

### Technique 5: Use Placeholders
Replace: Specific details with "[COMPANY]", "[AMOUNT]", "[DATE]"

---

## Industry-Specific Concerns

### Healthcare
- HIPAA regulations (US)
- Patient data is never appropriate
- Even de-identified data can be risky

### Legal
- Attorney-client privilege at stake
- Case details could compromise clients
- Court documents may be public but still sensitive

### Finance
- Insider trading concerns
- Customer financial data protected by law
- Regulatory compliance requirements

### HR
- Employee privacy rights
- Performance data protected
- Discrimination risk from AI analysis

---

## Company Policies

Before using AI at work, know your company's policy:

### Questions to Ask
- Is AI usage approved?
- Which AI tools are sanctioned?
- What data categories are prohibited?
- Is there an enterprise version with privacy controls?
- What are the consequences of policy violation?

### If There's No Policy
- Assume restrictive interpretation
- Raise the question with IT/leadership
- Use anonymized data only
- Document your careful approach

---

## Safe AI Usage Checklist

Before hitting "send," verify:

- [ ] No real names (people or companies)
- [ ] No identifying numbers (SSN, account numbers)
- [ ] No financial specifics
- [ ] No medical/health information
- [ ] No passwords or credentials
- [ ] No confidential business data
- [ ] No client/customer data
- [ ] Nothing that would violate policies
- [ ] Nothing you'd be embarrassed to explain

---

## Enterprise AI Solutions

If your company needs AI with sensitive data:

### Options That Exist
- Private cloud deployments
- On-premise LLM installations
- Enterprise agreements with no-training clauses
- Data residency guarantees

### Questions for Vendors
- Where is data processed?
- Is data used for training?
- Who has access to conversations?
- What are retention policies?
- What security certifications exist?

---

## The Third-Party Problem

Remember: It's not just YOUR data.

When you share:
- Client information
- Employee data
- Partner details
- Customer records

You're breaching THEIR trust, not just yours.

**Rule:** If the data belongs to someone else, assume you can't share it.

---

## Creating Safe Habits

### Habit 1: Pause Before Pasting
Stop. Read what you're about to share. Remove sensitive elements.

### Habit 2: Work in Two Windows
Draft in a local document. Paste anonymized versions to AI.

### Habit 3: Use Generic Examples
Instead of real data, create fictional examples that serve the same purpose.

### Habit 4: Check Settings
Know what privacy options your AI tool offers. Enable them.

### Habit 5: Assume Public
Act as if every prompt could become public. It might.

---

## The Data Privacy Mindset

**Think of AI as a smart intern.**

Would you hand an intern:
- The company's financial records?
- Customer Social Security numbers?
- Confidential strategy documents?

Probably not. Apply the same judgment to AI.

---

## Exercise: Privacy Audit

Take a conversation you've had with AI recently:

1. Review what you shared
2. Identify any sensitive information
3. Consider: Could this cause harm if exposed?
4. Rewrite the prompts with proper anonymization

This builds the habit of thinking before sharing.

---

## Key Takeaways

1. **Assume data may be stored and seen** by others
2. **Never share PII** - names, numbers, addresses
3. **Anonymize before asking** - remove identifying details
4. **Know your company's policy** - and follow it
5. **Protect others' data** - clients, employees, partners
6. **Use the newspaper test** - if it would be embarrassing, don't type it

---

## What's Next?

You understand hallucinations and data privacy. Now it's time to put everything together.

In the final lesson, you'll take the **AI Literacy Quiz** to test your knowledge and earn your certification.

---

*Module 4: Safety First (Lesson 2 of 3)*
